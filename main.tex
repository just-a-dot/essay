%
% File coling2016.tex
%
% Contact: mutiyama@nict.go.jp
%%
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[backend=biber, style=authoryear-comp]{biblatex}

\bibliography{sources}

%\setcitestyle{authoryear,open={((},close={))}}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Audio compression using autoencoders}

\author{Fran\c{c}ois Beuvin \\
  Chalmers \\
  \And
  Jan Eisenmenger \\
  University of Gothenburg \\
  \And
  Emil Jinstrand \\
  Chalmers \\
  \AND
  Arianna Masciolini \\
  University of Gothenburg\\
  \And
  \r{A}sa Westlund \\
  Gothenburg University\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Abstract
\end{abstract}

\nocite{*}



\section{Introduction}
\begin{itemize}
    \item introduce audiocompression
    \item how does it normally work
    \item state why autoencoders work differently
    \item structure of the essay
\end{itemize}

\section{Approaches to Autoencoders}
% general information about project, such as pre and postprocessing
\begin{itemize}
    \item what is a wave file
    \item did we get our data from
    \item pre & post processing
\end{itemize}
    	
   
\subsection{Basic Autoencoders}
http://science.sciencemag.org/content/313/5786/504

[]https://ramhiser.com/post/2018-05-14-autoencoders-with-keras/]

Autoencocers compressors are a type of unsupervised learning where the training data is used both as input and output data in a neural network, sometimes these models are also refferd to as self-supervised since the model creates its own lables. (\cite{Hinton.2006}) [https://ramhiser.com/post/2018-05-14-autoencoders-with-keras/]Autoencocers compressors are a type of unsupervised learning where the training data is used both as input and output data in a neural network. In the most stimple form there is just one hidden layer. The weights and bias:es in the in the hidden layer can be used to decode data and the weights and biases in the output layer can be used to encode the data. If the size of the hidden layer is smaller than the size of the input and output data. We can use the weights and biases in the hidden layer to compress the data and weights and biases in the outputlayer to decompress the data. 
One thing to notice when using autoencocers for compression is that even if good results can be achieved on training data the model will not work at all on data that is very different from the training data.[ https://blog.usejournal.com/auto-encoders-for-data-compression-73ea0724e90b] 
\paragraph{Applications}
\paragraph{Implementation}
Since the basic one
the 2:nd layer is just the size of the compressed data.
In our implementation we use samples of size 22050, since this size is very ram-demanding we train our network with 10\% of the sample size.
we have a neural network where the input layer is 2205, the compressed layer is 220, and the ouput layer is back to 2205.


\subsection{Asymmetrical Autoencoders}

\paragraph{General Approach}

Traditionally autoencoders are symmetrical autoencoders, this means the same number of encoders and decoders layers, but itâ€™s not a necessity, we can use asymmetrical autoencoders.
So the size of the encoders and decoders layers are different especially in our models we have choose to have just one decoders layers\\
By reducing the number of decoder layers, we are reducing the number of parameters, network weights, to learn. This means that with limited training data, we will have fewer parameters, multiple encoders and single decoder, to learn. This in turn is likely to reduce over-fitting, improve generalizability and increase classification accuracy. \\
At each decoder, the reconstruction incurs a loss. By reducing the number of decoder layers, we minimize the loss
Some studies shows that for benchmark problems asymmetric stacked autoencoders outperform symmetric stacked autoencoders for both classification and compression problems. \\
So we have try, test the two architecture with the same parameters, and our practical results correspond to the theory, for this task, asymmetricals autoencoders have better results
  
\paragraph{Applications} 
\paragraph{Implementation} 

source : \\
https://arxiv.org/pdf/1711.08352.pdf \\
https://ieeexplore.ieee.org/abstract/document/7965949 \\
https://pdfs.semanticscholar.org/e74d/00a0dc0049488c958eeb1dfbcf8abf4f6928.pdf \\

\subsection{Time-Convolutional Autoencoders}
\paragraph{General Approach}
\paragraph{Applications}
\paragraph{Implementation}
\subsection{Sequence-to-Sequence Autoencoders}
As mentioned in the previous section, a wave file is essentially to be considered a sequence of floating point numbers. As a consequence, it can be useful to experiment with a model capable of capturing temporal structure. Looking at the problem from this perspective, the obvious choice is to build an autoencoder out of two Recurrent Neural Networks (RNNs). RNNs, i.e. neural networks where connections between nodes form a directed graph along a temporal sequence are, in fact, the natural generalization of feed-forward neural networks to handle sequences. In principle, as such RNNs are provided with all the relevant information, this approach would work. In practice, however, training an autoencoder made of RNNs would prove itself to be in general difficult due to long range temporal dependencies, as it has been shown (proven?) that gradient descent becomes increasingly inefficient as the temporal span of the dependencies increases. In order to solve this problem, Long Short Term Memory (LSTM), a RNN architecture specifically designed to support such sequence data, has been proposed. Autoencoders made of a RNN encoder and a RNN decoder equipped with LSTM units, which we refer to as sequence to sequence autoencoders (SA), have been successfully implemented to solve a variety of representation learning tasks that need the learning to be unsupervised due to the lack of available labelled data, such as video representation learning, one of the earliest applications of SA, and to address various Natural Language Processing (NLP) problems, such as audio data representation, long text generation and machine translation. In all of these scenarios, one of the key advantages of SA is the possibility to encode input sequences of arbitrary length into a fixed-dimensionality array, something that is certainly also useful for our audio compression problem.
\section{Comparison}
\begin{itemize}
    \item comparison (with table)
    \item (observations on best model)
\end{itemize}

\section{Conclusion}


%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
% 

The following instructions are directed to authors of papers submitted
to COLING-2016 or accepted for publication in its proceedings. All
authors are required to adhere to these specifications. Authors are
required to provide a Portable Document Format (PDF) version of their
papers. \textbf{The proceedings are designed for printing on A4
  paper.}

Authors from countries in which access to word-processing systems is
limited should contact the publication chairs,
Hitoshi Isahara and Masao Utiyama
(\texttt{isahara@tut.jp, mutiyama@nict.go.jp}),
as soon as possible.

We will make additional instructions available at ``Instructions for authors'' section of 
\url{http://coling2016.anlp.jp}. Please check
this website regularly.

\section{General Instructions}
\subsection{Layout}
\label{ssec:layout}

Format manuscripts with a single column to a page, in the manner these
instructions are formatted. The exact dimensions for a page on A4
paper are:

testttttttt

\begin{itemize}
\item Left and right margins: 2.5 cm
\item Top margin: 2.5 cm
\item Bottom margin: 2.5 cm
\item Width: 16.0 cm
\item Height: 24.7 cm
\end{itemize}

\noindent Papers should not be submitted on any other paper size.
If you cannot meet the above requirements for
the production of your electronic submission, please contact the
publication chairs above as soon as possible.


\subsection{Fonts}

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble. If Times Roman is unavailable, use {\bf Computer
  Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
  10\% less dense than Adobe's Times Roman font.

The {\bf Times New Roman} font, which is configured for us in the
Microsoft Word template (coling2016.dot) and which some Linux
distributions offer for installation, can be used as well.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
captions & 11 pt & \\
sub-captions & 9 pt & \\
abstract text & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

\begin{itemize}
    %
    % % final paper: en-uk version
    %
    \item  This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}
           
    %
    % % final paper: en-us version
    %
    \item This work is licensed under a Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}
           
           
           
\end{itemize}

\section*{Acknowledgements}

The acknowledgements should go immediately before the references.  Do
not number the acknowledgements section. Do not include this section
when submitting your paper for review.

% include your own bib file like this:
\printbibliography

\end{document}